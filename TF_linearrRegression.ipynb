{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helli = tf.constant(\"Hello, Tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow의 구동원리\n",
    "\n",
    "- Tensorflow는 tensor라고하는 데이터(edge)가 operation등이 들어있는 Node를 통해 흐른다고 하여 Tensorflow라고한다. 따라서 Edge와 Node가 있는 Graph로 작동한다.\n",
    "- 따라서 위와같이 tf.constant를 해서 NOde를 만들더라도 단순히 print하여도 원하는 값이 나오지 않고,node의 실행을 위한 Session이 필요하다. \n",
    "- 따라서 먼저 Graph를 구성하고 Session을 통해서 Graph를 실행하는 것이 가장 큰 틀이라고 할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, Tensorflow'\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(helli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0)\n",
    "node3 = tf.add(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1 :  Tensor(\"Const_1:0\", shape=(), dtype=float32) node2 :  Tensor(\"Const_2:0\", shape=(), dtype=float32) node3 :  Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"node1 : \", node1, \"node2 : \", node2, \"node3 : \", node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run(node1, node2):  [3.0, 4.0]\n",
      "sess.run(node3) :  7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(\"sess.run(node1, node2): \", sess.run([node1, node2]))\n",
    "print(\"sess.run(node3) : \", sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b\n",
    "\n",
    "print(sess.run(adder_node, feed_dict = {a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, feed_dict = {a: [1, 3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hypothesis and cost function\n",
    "- H(x) = Wx + b\n",
    "- minimize cost(W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. H(x) = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "#Variable은 tensorflow가 사용하는 Variable이다\n",
    "#tf.random_normal([1]) rank가 1인(scalar) 랜덤한 숫자를 만든다.\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "#Our hypothesis XW + b\n",
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. cost(W,b) = 1/m*summation(H(x)-y)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cost/Loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.460292 [ 0.90256619] [-0.47889996]\n",
      "20 0.0213899 [ 1.12858021] [-0.35898384]\n",
      "40 0.0158303 [ 1.1436224] [-0.33283764]\n",
      "60 0.0143447 [ 1.13888049] [-0.31631243]\n",
      "80 0.0130278 [ 1.13254464] [-0.30136275]\n",
      "100 0.0118321 [ 1.12633371] [-0.28719181]\n",
      "120 0.0107461 [ 1.12039816] [-0.27369407]\n",
      "140 0.00975976 [ 1.11474013] [-0.26083139]\n",
      "160 0.00886396 [ 1.10934782] [-0.24857323]\n",
      "180 0.00805039 [ 1.10420883] [-0.23689124]\n",
      "200 0.00731151 [ 1.09931147] [-0.22575827]\n",
      "220 0.00664043 [ 1.09464419] [-0.21514843]\n",
      "240 0.00603094 [ 1.09019625] [-0.20503725]\n",
      "260 0.0054774 [ 1.08595729] [-0.19540125]\n",
      "280 0.00497466 [ 1.08191776] [-0.1862182]\n",
      "300 0.00451807 [ 1.07806778] [-0.17746663]\n",
      "320 0.00410339 [ 1.07439899] [-0.16912636]\n",
      "340 0.00372675 [ 1.07090235] [-0.16117802]\n",
      "360 0.0033847 [ 1.06757033] [-0.15360326]\n",
      "380 0.00307404 [ 1.06439471] [-0.14638443]\n",
      "400 0.00279189 [ 1.06136847] [-0.13950488]\n",
      "420 0.00253564 [ 1.0584842] [-0.13294852]\n",
      "440 0.0023029 [ 1.05573559] [-0.12670034]\n",
      "460 0.00209153 [ 1.05311632] [-0.12074587]\n",
      "480 0.00189955 [ 1.05061996] [-0.11507123]\n",
      "500 0.00172521 [ 1.04824114] [-0.10966329]\n",
      "520 0.00156687 [ 1.0459739] [-0.10450953]\n",
      "540 0.00142305 [ 1.04381323] [-0.09959796]\n",
      "560 0.00129244 [ 1.04175448] [-0.09491728]\n",
      "580 0.00117382 [ 1.03979218] [-0.09045664]\n",
      "600 0.00106608 [ 1.03792191] [-0.08620547]\n",
      "620 0.000968228 [ 1.03613961] [-0.08215407]\n",
      "640 0.000879359 [ 1.03444123] [-0.07829311]\n",
      "660 0.000798647 [ 1.03282261] [-0.07461358]\n",
      "680 0.000725349 [ 1.03128016] [-0.07110701]\n",
      "700 0.000658773 [ 1.02981007] [-0.0677653]\n",
      "720 0.000598309 [ 1.02840912] [-0.06458056]\n",
      "740 0.000543391 [ 1.02707386] [-0.06154548]\n",
      "760 0.000493515 [ 1.02580142] [-0.05865299]\n",
      "780 0.000448218 [ 1.02458906] [-0.05589651]\n",
      "800 0.000407079 [ 1.02343333] [-0.05326957]\n",
      "820 0.000369714 [ 1.02233219] [-0.05076611]\n",
      "840 0.000335785 [ 1.02128279] [-0.04838037]\n",
      "860 0.000304964 [ 1.02028263] [-0.04610683]\n",
      "880 0.000276976 [ 1.01932919] [-0.04394007]\n",
      "900 0.000251554 [ 1.01842082] [-0.04187496]\n",
      "920 0.000228463 [ 1.01755512] [-0.03990697]\n",
      "940 0.000207493 [ 1.01673019] [-0.03803151]\n",
      "960 0.000188449 [ 1.01594388] [-0.03624418]\n",
      "980 0.000171153 [ 1.01519454] [-0.0345408]\n",
      "1000 0.000155443 [ 1.01448047] [-0.03291752]\n",
      "1020 0.000141176 [ 1.01380002] [-0.03137056]\n",
      "1040 0.00012822 [ 1.01315141] [-0.02989629]\n",
      "1060 0.000116451 [ 1.01253331] [-0.02849124]\n",
      "1080 0.000105762 [ 1.01194429] [-0.02715223]\n",
      "1100 9.60547e-05 [ 1.01138294] [-0.02587615]\n",
      "1120 8.7238e-05 [ 1.01084805] [-0.02466005]\n",
      "1140 7.92304e-05 [ 1.01033819] [-0.02350111]\n",
      "1160 7.19587e-05 [ 1.00985217] [-0.02239662]\n",
      "1180 6.53538e-05 [ 1.00938916] [-0.02134399]\n",
      "1200 5.93554e-05 [ 1.00894797] [-0.02034086]\n",
      "1220 5.39077e-05 [ 1.00852752] [-0.01938493]\n",
      "1240 4.89598e-05 [ 1.00812674] [-0.01847392]\n",
      "1260 4.44658e-05 [ 1.00774479] [-0.01760574]\n",
      "1280 4.0384e-05 [ 1.00738084] [-0.01677836]\n",
      "1300 3.66792e-05 [ 1.00703394] [-0.01598987]\n",
      "1320 3.33117e-05 [ 1.00670338] [-0.01523844]\n",
      "1340 3.02543e-05 [ 1.00638843] [-0.01452228]\n",
      "1360 2.74773e-05 [ 1.00608814] [-0.01383977]\n",
      "1380 2.49554e-05 [ 1.00580204] [-0.01318937]\n",
      "1400 2.26649e-05 [ 1.0055294] [-0.01256955]\n",
      "1420 2.05847e-05 [ 1.00526953] [-0.01197882]\n",
      "1440 1.86953e-05 [ 1.00502181] [-0.01141587]\n",
      "1460 1.69797e-05 [ 1.0047859] [-0.01087938]\n",
      "1480 1.54212e-05 [ 1.00456095] [-0.01036808]\n",
      "1500 1.40056e-05 [ 1.00434661] [-0.0098808]\n",
      "1520 1.27204e-05 [ 1.00414228] [-0.00941646]\n",
      "1540 1.15526e-05 [ 1.00394762] [-0.00897391]\n",
      "1560 1.04922e-05 [ 1.00376213] [-0.00855216]\n",
      "1580 9.52925e-06 [ 1.00358534] [-0.00815026]\n",
      "1600 8.65475e-06 [ 1.0034169] [-0.00776726]\n",
      "1620 7.8606e-06 [ 1.00325632] [-0.00740226]\n",
      "1640 7.13908e-06 [ 1.00310326] [-0.00705439]\n",
      "1660 6.48395e-06 [ 1.00295746] [-0.00672288]\n",
      "1680 5.88851e-06 [ 1.00281847] [-0.00640695]\n",
      "1700 5.34822e-06 [ 1.00268602] [-0.00610586]\n",
      "1720 4.85778e-06 [ 1.00255978] [-0.00581896]\n",
      "1740 4.41152e-06 [ 1.0024395] [-0.00554551]\n",
      "1760 4.00655e-06 [ 1.00232494] [-0.00528492]\n",
      "1780 3.6391e-06 [ 1.00221562] [-0.00503658]\n",
      "1800 3.30534e-06 [ 1.00211155] [-0.00479993]\n",
      "1820 3.0019e-06 [ 1.00201237] [-0.00457443]\n",
      "1840 2.72644e-06 [ 1.00191784] [-0.00435949]\n",
      "1860 2.47644e-06 [ 1.00182784] [-0.0041547]\n",
      "1880 2.24914e-06 [ 1.00174189] [-0.0039595]\n",
      "1900 2.04263e-06 [ 1.00165987] [-0.00377346]\n",
      "1920 1.85541e-06 [ 1.00158215] [-0.00359618]\n",
      "1940 1.68489e-06 [ 1.00150764] [-0.00342724]\n",
      "1960 1.53038e-06 [ 1.00143695] [-0.00326622]\n",
      "1980 1.38995e-06 [ 1.00136936] [-0.00311278]\n",
      "2000 1.26242e-06 [ 1.0013051] [-0.00296654]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#fit the Line\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    # 경사 하강법 20회에 한번씩 출력\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.655782 [ 0.90261871] [ 0.59413648]\n",
      "20 0.0314573 [ 1.11412919] [ 0.68408448]\n",
      "40 0.0274608 [ 1.10758126] [ 0.71157986]\n",
      "60 0.0239818 [ 1.10053992] [ 0.73701894]\n",
      "80 0.0209434 [ 1.09395552] [ 0.76079077]\n",
      "100 0.0182901 [ 1.08780217] [ 0.78300583]\n",
      "120 0.0159729 [ 1.08205199] [ 0.80376601]\n",
      "140 0.0139492 [ 1.0766784] [ 0.82316661]\n",
      "160 0.012182 [ 1.07165682] [ 0.84129655]\n",
      "180 0.0106386 [ 1.06696379] [ 0.85823929]\n",
      "200 0.00929076 [ 1.06257832] [ 0.87407237]\n",
      "220 0.00811369 [ 1.05848002] [ 0.88886851]\n",
      "240 0.00708577 [ 1.05465019] [ 0.9026956]\n",
      "260 0.00618805 [ 1.05107105] [ 0.91561723]\n",
      "280 0.00540406 [ 1.04772639] [ 0.92769259]\n",
      "300 0.0047194 [ 1.04460073] [ 0.93897718]\n",
      "320 0.00412149 [ 1.04167986] [ 0.94952273]\n",
      "340 0.00359933 [ 1.03895009] [ 0.95937759]\n",
      "360 0.00314332 [ 1.03639925] [ 0.9685871]\n",
      "380 0.00274508 [ 1.03401542] [ 0.97719347]\n",
      "400 0.0023973 [ 1.03178775] [ 0.98523617]\n",
      "420 0.00209358 [ 1.02970588] [ 0.99275219]\n",
      "440 0.00182834 [ 1.02776051] [ 0.99977595]\n",
      "460 0.0015967 [ 1.02594244] [ 1.00633967]\n",
      "480 0.00139441 [ 1.02424335] [ 1.01247358]\n",
      "500 0.00121776 [ 1.02265573] [ 1.01820552]\n",
      "520 0.00106348 [ 1.02117193] [ 1.02356231]\n",
      "540 0.000928735 [ 1.01978528] [ 1.02856863]\n",
      "560 0.000811073 [ 1.0184896] [ 1.03324676]\n",
      "580 0.000708317 [ 1.01727867] [ 1.03761828]\n",
      "600 0.000618577 [ 1.01614714] [ 1.0417037]\n",
      "620 0.000540206 [ 1.01508963] [ 1.0455215]\n",
      "640 0.000471757 [ 1.01410127] [ 1.04908991]\n",
      "660 0.000411989 [ 1.01317775] [ 1.05242407]\n",
      "680 0.000359794 [ 1.01231468] [ 1.05553997]\n",
      "700 0.000314213 [ 1.01150823] [ 1.05845141]\n",
      "720 0.000274404 [ 1.0107547] [ 1.06117225]\n",
      "740 0.000239641 [ 1.01005018] [ 1.0637151]\n",
      "760 0.000209281 [ 1.00939202] [ 1.06609142]\n",
      "780 0.000182766 [ 1.00877702] [ 1.06831217]\n",
      "800 0.000159606 [ 1.00820208] [ 1.07038784]\n",
      "820 0.000139382 [ 1.00766492] [ 1.07232726]\n",
      "840 0.000121728 [ 1.00716293] [ 1.07413948]\n",
      "860 0.000106307 [ 1.00669384] [ 1.07583296]\n",
      "880 9.28381e-05 [ 1.00625551] [ 1.07741559]\n",
      "900 8.1077e-05 [ 1.0058459] [ 1.07889485]\n",
      "920 7.08023e-05 [ 1.00546288] [ 1.0802772]\n",
      "940 6.1831e-05 [ 1.00510514] [ 1.08156884]\n",
      "960 5.39987e-05 [ 1.00477076] [ 1.08277583]\n",
      "980 4.71571e-05 [ 1.00445831] [ 1.08390391]\n",
      "1000 4.11845e-05 [ 1.00416636] [ 1.08495796]\n",
      "1020 3.59653e-05 [ 1.00389349] [ 1.08594298]\n",
      "1040 3.14098e-05 [ 1.00363863] [ 1.08686352]\n",
      "1060 2.74309e-05 [ 1.00340033] [ 1.08772373]\n",
      "1080 2.39557e-05 [ 1.00317764] [ 1.0885278]\n",
      "1100 2.09213e-05 [ 1.0029695] [ 1.08927882]\n",
      "1120 1.82714e-05 [ 1.00277507] [ 1.08998096]\n",
      "1140 1.59568e-05 [ 1.0025934] [ 1.09063685]\n",
      "1160 1.39355e-05 [ 1.00242364] [ 1.09125018]\n",
      "1180 1.21693e-05 [ 1.00226486] [ 1.09182322]\n",
      "1200 1.06279e-05 [ 1.00211656] [ 1.09235859]\n",
      "1220 9.28168e-06 [ 1.00197792] [ 1.09285903]\n",
      "1240 8.10508e-06 [ 1.00184834] [ 1.09332681]\n",
      "1260 7.07875e-06 [ 1.00172734] [ 1.09376371]\n",
      "1280 6.1819e-06 [ 1.00161433] [ 1.09417212]\n",
      "1300 5.39866e-06 [ 1.00150847] [ 1.09455395]\n",
      "1320 4.71492e-06 [ 1.00140977] [ 1.09491038]\n",
      "1340 4.11766e-06 [ 1.00131738] [ 1.09524357]\n",
      "1360 3.59601e-06 [ 1.00123131] [ 1.09555495]\n",
      "1380 3.14047e-06 [ 1.00115061] [ 1.09584606]\n",
      "1400 2.74291e-06 [ 1.00107527] [ 1.09611809]\n",
      "1420 2.39511e-06 [ 1.00100482] [ 1.09637225]\n",
      "1440 2.09216e-06 [ 1.00093901] [ 1.09660959]\n",
      "1460 1.82708e-06 [ 1.00087762] [ 1.09683168]\n",
      "1480 1.59553e-06 [ 1.00082016] [ 1.09703922]\n",
      "1500 1.39323e-06 [ 1.0007664] [ 1.09723306]\n",
      "1520 1.21682e-06 [ 1.00071621] [ 1.09741426]\n",
      "1540 1.06286e-06 [ 1.00066936] [ 1.09758353]\n",
      "1560 9.28156e-07 [ 1.00062549] [ 1.09774172]\n",
      "1580 8.10606e-07 [ 1.00058448] [ 1.09788954]\n",
      "1600 7.08001e-07 [ 1.00054622] [ 1.09802771]\n",
      "1620 6.18319e-07 [ 1.00051045] [ 1.09815681]\n",
      "1640 5.40015e-07 [ 1.00047696] [ 1.09827745]\n",
      "1660 4.71685e-07 [ 1.00044584] [ 1.09839022]\n",
      "1680 4.11965e-07 [ 1.00041676] [ 1.0984956]\n",
      "1700 3.59831e-07 [ 1.00038946] [ 1.09859407]\n",
      "1720 3.14141e-07 [ 1.00036395] [ 1.09868622]\n",
      "1740 2.74402e-07 [ 1.00033998] [ 1.09877205]\n",
      "1760 2.39712e-07 [ 1.00031793] [ 1.09885252]\n",
      "1780 2.09333e-07 [ 1.00029707] [ 1.09892762]\n",
      "1800 1.82751e-07 [ 1.00027776] [ 1.09899771]\n",
      "1820 1.59696e-07 [ 1.00025952] [ 1.09906328]\n",
      "1840 1.39509e-07 [ 1.00024259] [ 1.09912443]\n",
      "1860 1.21872e-07 [ 1.00022662] [ 1.09918165]\n",
      "1880 1.06393e-07 [ 1.00021195] [ 1.09923518]\n",
      "1900 9.29881e-08 [ 1.00019789] [ 1.09928524]\n",
      "1920 8.12308e-08 [ 1.00018501] [ 1.09933197]\n",
      "1940 7.09837e-08 [ 1.00017297] [ 1.09937561]\n",
      "1960 6.19625e-08 [ 1.00016153] [ 1.09941661]\n",
      "1980 5.40682e-08 [ 1.00015104] [ 1.09945488]\n",
      "2000 4.72418e-08 [ 1.00014126] [ 1.0994904]\n"
     ]
    }
   ],
   "source": [
    "#placeholders를 사용해봅시다\n",
    "#placeholders를 이용하면 먼저 알고리즘을 만들고 나중에 데이터를 집어넣기 용이하다. \n",
    "#Shape = [None]는 몇차원데이터든 상관없다 이경우 2\n",
    "X = tf.placeholder(tf.float32, shape = [None])\n",
    "Y = tf.placeholder(tf.float32, shape = [None])\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "#Our hypothesis XW + b\n",
    "hypothesis = X * W + b\n",
    "#cost/Loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "#Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train], feed_dict={X: [1, 2, 3, 4, 5], Y: [2.1, 3.1, 4.1, 5.1, 6.1]})\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost_val, W_val, b_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.10019588]\n",
      "[ 3.59984398]\n",
      "[ 2.59970331  4.59998512]\n"
     ]
    }
   ],
   "source": [
    "# testing our model\n",
    "print(sess.run(hypothesis, feed_dict = {X: [5]}))\n",
    "print(sess.run(hypothesis, feed_dict = {X: [2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict = {X: [1.5, 3.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing Cost\n",
    "- gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19d5f84ab00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "W= tf.placeholder(tf.float32)\n",
    "hypothesis = X * W\n",
    "#cost function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "#Launch the graph in the graph\n",
    "sess = tf.Session()\n",
    "#Initialize global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#비용함수 그리기 위한 변수들을 위한 list 생성\n",
    "W_val = []\n",
    "cost_val = []\n",
    "for i in range(-30, 50):\n",
    "    #(-3,5)구간에서 plotting\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "    \n",
    "# 비용함수 plotting\n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강법\n",
    "경사를 이용하기 위해 미분한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# W -= Learning_rate * derivative of cost function\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "#tensorflow 에서는 -=를 통해서 업데이트가 안되므로 assigin()이용\n",
    "update = W.assign(descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0277496 [ 0.92288744]\n",
      "1 0.00789324 [ 0.95887327]\n",
      "2 0.00224519 [ 0.97806573]\n",
      "3 0.000638636 [ 0.98830169]\n",
      "4 0.000181657 [ 0.99376088]\n",
      "5 5.16716e-05 [ 0.99667245]\n",
      "6 1.4697e-05 [ 0.99822533]\n",
      "7 4.18088e-06 [ 0.99905348]\n",
      "8 1.1892e-06 [ 0.99949521]\n",
      "9 3.38305e-07 [ 0.99973077]\n",
      "10 9.62316e-08 [ 0.99985641]\n",
      "11 2.73853e-08 [ 0.99992341]\n",
      "12 7.7843e-09 [ 0.99995917]\n",
      "13 2.21138e-09 [ 0.99997824]\n",
      "14 6.29045e-10 [ 0.99998838]\n",
      "15 1.79322e-10 [ 0.9999938]\n",
      "16 4.97629e-11 [ 0.99999672]\n",
      "17 1.41505e-11 [ 0.99999827]\n",
      "18 3.62495e-12 [ 0.99999911]\n",
      "19 1.06108e-12 [ 0.99999952]\n",
      "20 2.65269e-13 [ 0.99999976]\n"
     ]
    }
   ],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "W= tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "# our Hypothesis for linear model (simplified) X * W\n",
    "hypothesis = X * W\n",
    "# cost/ Loss Function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "#minimize \n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "#launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# initialize global function variable in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict = {X: x_data, Y: y_data})\n",
    "    print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow에서의 GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 70.0\n",
      "1 5.6\n",
      "2 1.30667\n",
      "3 1.02044\n",
      "4 1.00136\n",
      "5 1.00009\n",
      "6 1.00001\n",
      "7 1.0\n",
      "8 1.0\n",
      "9 1.0\n",
      "10 1.0\n",
      "11 1.0\n",
      "12 1.0\n",
      "13 1.0\n",
      "14 1.0\n",
      "15 1.0\n",
      "16 1.0\n",
      "17 1.0\n",
      "18 1.0\n",
      "19 1.0\n",
      "20 1.0\n",
      "21 1.0\n",
      "22 1.0\n",
      "23 1.0\n",
      "24 1.0\n",
      "25 1.0\n",
      "26 1.0\n",
      "27 1.0\n",
      "28 1.0\n",
      "29 1.0\n",
      "30 1.0\n",
      "31 1.0\n",
      "32 1.0\n",
      "33 1.0\n",
      "34 1.0\n",
      "35 1.0\n",
      "36 1.0\n",
      "37 1.0\n",
      "38 1.0\n",
      "39 1.0\n",
      "40 1.0\n",
      "41 1.0\n",
      "42 1.0\n",
      "43 1.0\n",
      "44 1.0\n",
      "45 1.0\n",
      "46 1.0\n",
      "47 1.0\n",
      "48 1.0\n",
      "49 1.0\n",
      "50 1.0\n",
      "51 1.0\n",
      "52 1.0\n",
      "53 1.0\n",
      "54 1.0\n",
      "55 1.0\n",
      "56 1.0\n",
      "57 1.0\n",
      "58 1.0\n",
      "59 1.0\n",
      "60 1.0\n",
      "61 1.0\n",
      "62 1.0\n",
      "63 1.0\n",
      "64 1.0\n",
      "65 1.0\n",
      "66 1.0\n",
      "67 1.0\n",
      "68 1.0\n",
      "69 1.0\n",
      "70 1.0\n",
      "71 1.0\n",
      "72 1.0\n",
      "73 1.0\n",
      "74 1.0\n",
      "75 1.0\n",
      "76 1.0\n",
      "77 1.0\n",
      "78 1.0\n",
      "79 1.0\n",
      "80 1.0\n",
      "81 1.0\n",
      "82 1.0\n",
      "83 1.0\n",
      "84 1.0\n",
      "85 1.0\n",
      "86 1.0\n",
      "87 1.0\n",
      "88 1.0\n",
      "89 1.0\n",
      "90 1.0\n",
      "91 1.0\n",
      "92 1.0\n",
      "93 1.0\n",
      "94 1.0\n",
      "95 1.0\n",
      "96 1.0\n",
      "97 1.0\n",
      "98 1.0\n",
      "99 1.0\n"
     ]
    }
   ],
   "source": [
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "W = tf.Variable(70.0)\n",
    "hypothesis = X * W\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "#tensorflow에서는 메소드로 존재한다. \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "                        \n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "                                \n",
    "for step in range(100):\n",
    "    print(step, sess.run(W))\n",
    "    sess.run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [37.333332, 5.0, [(37.333332, 5.0)]]\n",
      "1 [33.848888, 4.6266665, [(33.848888, 4.6266665)]]\n",
      "2 [30.689655, 4.2881775, [(30.689655, 4.2881775)]]\n",
      "3 [27.825289, 3.981281, [(27.825289, 3.981281)]]\n",
      "4 [25.228264, 3.7030282, [(25.228264, 3.7030282)]]\n",
      "5 [22.873627, 3.4507456, [(22.873627, 3.4507456)]]\n",
      "6 [20.738754, 3.2220094, [(20.738754, 3.2220094)]]\n",
      "7 [18.803141, 3.014622, [(18.803141, 3.014622)]]\n",
      "8 [17.04818, 2.8265905, [(17.048178, 2.8265905)]]\n",
      "9 [15.457016, 2.6561089, [(15.457016, 2.6561089)]]\n",
      "10 [14.014363, 2.5015388, [(14.014362, 2.5015388)]]\n",
      "11 [12.706355, 2.3613951, [(12.706355, 2.3613951)]]\n",
      "12 [11.520428, 2.2343316, [(11.520428, 2.2343316)]]\n",
      "13 [10.445188, 2.1191273, [(10.445188, 2.1191273)]]\n",
      "14 [9.4703045, 2.0146754, [(9.4703045, 2.0146754)]]\n",
      "15 [8.5864086, 1.9199723, [(8.5864077, 1.9199723)]]\n",
      "16 [7.7850103, 1.8341082, [(7.7850103, 1.8341082)]]\n",
      "17 [7.0584097, 1.7562581, [(7.0584097, 1.7562581)]]\n",
      "18 [6.3996248, 1.6856741, [(6.3996248, 1.6856741)]]\n",
      "19 [5.8023272, 1.6216779, [(5.8023272, 1.6216779)]]\n",
      "20 [5.2607765, 1.5636547, [(5.2607765, 1.5636547)]]\n",
      "21 [4.7697711, 1.5110469, [(4.7697711, 1.5110469)]]\n",
      "22 [4.3245926, 1.4633492, [(4.3245926, 1.4633492)]]\n",
      "23 [3.9209647, 1.4201033, [(3.9209647, 1.4201033)]]\n",
      "24 [3.5550079, 1.3808937, [(3.5550079, 1.3808937)]]\n",
      "25 [3.223207, 1.3453436, [(3.223207, 1.3453436)]]\n",
      "26 [2.9223742, 1.3131115, [(2.9223742, 1.3131115)]]\n",
      "27 [2.6496186, 1.2838877, [(2.6496186, 1.2838877)]]\n",
      "28 [2.4023216, 1.2573916, [(2.4023218, 1.2573916)]]\n",
      "29 [2.1781051, 1.2333684, [(2.1781051, 1.2333684)]]\n",
      "30 [1.9748147, 1.2115873, [(1.9748147, 1.2115873)]]\n",
      "31 [1.7904994, 1.1918392, [(1.7904994, 1.1918392)]]\n",
      "32 [1.6233861, 1.1739342, [(1.6233861, 1.1739342)]]\n",
      "33 [1.4718704, 1.1577004, [(1.4718704, 1.1577004)]]\n",
      "34 [1.3344965, 1.1429818, [(1.3344965, 1.1429818)]]\n",
      "35 [1.2099432, 1.1296368, [(1.2099432, 1.1296368)]]\n",
      "36 [1.0970153, 1.1175374, [(1.0970153, 1.1175374)]]\n",
      "37 [0.99462759, 1.1065673, [(0.99462759, 1.1065673)]]\n",
      "38 [0.90179634, 1.096621, [(0.90179634, 1.096621)]]\n",
      "39 [0.81762886, 1.0876031, [(0.81762886, 1.0876031)]]\n",
      "40 [0.74131644, 1.0794268, [(0.74131644, 1.0794268)]]\n",
      "41 [0.67212707, 1.0720136, [(0.67212707, 1.0720136)]]\n",
      "42 [0.60939533, 1.0652924, [(0.60939533, 1.0652924)]]\n",
      "43 [0.55251819, 1.0591984, [(0.55251819, 1.0591984)]]\n",
      "44 [0.50094914, 1.0536731, [(0.50094914, 1.0536731)]]\n",
      "45 [0.45419377, 1.0486636, [(0.45419377, 1.0486636)]]\n",
      "46 [0.41180158, 1.0441216, [(0.41180158, 1.0441216)]]\n",
      "47 [0.37336725, 1.0400037, [(0.37336725, 1.0400037)]]\n",
      "48 [0.33851996, 1.03627, [(0.33851999, 1.03627)]]\n",
      "49 [0.30692515, 1.0328848, [(0.30692515, 1.0328848)]]\n",
      "50 [0.27827826, 1.0298156, [(0.27827826, 1.0298156)]]\n",
      "51 [0.25230527, 1.0270327, [(0.25230527, 1.0270327)]]\n",
      "52 [0.2287569, 1.0245097, [(0.2287569, 1.0245097)]]\n",
      "53 [0.20740572, 1.022222, [(0.20740572, 1.022222)]]\n",
      "54 [0.18804836, 1.020148, [(0.18804836, 1.020148)]]\n",
      "55 [0.17049654, 1.0182675, [(0.17049654, 1.0182675)]]\n",
      "56 [0.15458433, 1.0165626, [(0.15458435, 1.0165626)]]\n",
      "57 [0.14015675, 1.0150168, [(0.14015675, 1.0150168)]]\n",
      "58 [0.12707591, 1.0136153, [(0.12707591, 1.0136153)]]\n",
      "59 [0.11521538, 1.0123445, [(0.11521538, 1.0123445)]]\n",
      "60 [0.10446167, 1.0111923, [(0.10446167, 1.0111923)]]\n",
      "61 [0.094712019, 1.0101477, [(0.094712019, 1.0101477)]]\n",
      "62 [0.085872017, 1.0092006, [(0.085872017, 1.0092006)]]\n",
      "63 [0.077858053, 1.0083419, [(0.077858053, 1.0083419)]]\n",
      "64 [0.070591293, 1.0075634, [(0.070591293, 1.0075634)]]\n",
      "65 [0.064002357, 1.0068574, [(0.064002357, 1.0068574)]]\n",
      "66 [0.05802846, 1.0062174, [(0.05802846, 1.0062174)]]\n",
      "67 [0.052612223, 1.005637, [(0.052612223, 1.005637)]]\n",
      "68 [0.047702469, 1.005111, [(0.047702469, 1.005111)]]\n",
      "69 [0.043249767, 1.0046339, [(0.043249767, 1.0046339)]]\n",
      "70 [0.039213181, 1.0042014, [(0.039213181, 1.0042014)]]\n",
      "71 [0.035553534, 1.0038093, [(0.035553534, 1.0038093)]]\n",
      "72 [0.032236177, 1.0034539, [(0.032236177, 1.0034539)]]\n",
      "73 [0.029227655, 1.0031315, [(0.029227655, 1.0031315)]]\n",
      "74 [0.02649951, 1.0028392, [(0.02649951, 1.0028392)]]\n",
      "75 [0.024025917, 1.0025742, [(0.024025917, 1.0025742)]]\n",
      "76 [0.021783749, 1.002334, [(0.021783751, 1.002334)]]\n",
      "77 [0.01975123, 1.0021162, [(0.01975123, 1.0021162)]]\n",
      "78 [0.017907381, 1.0019187, [(0.017907381, 1.0019187)]]\n",
      "79 [0.016236702, 1.0017396, [(0.016236702, 1.0017396)]]\n",
      "80 [0.014720837, 1.0015773, [(0.014720837, 1.0015773)]]\n",
      "81 [0.013346991, 1.00143, [(0.013346991, 1.00143)]]\n",
      "82 [0.012100855, 1.0012965, [(0.012100855, 1.0012965)]]\n",
      "83 [0.010971785, 1.0011755, [(0.010971785, 1.0011755)]]\n",
      "84 [0.0099481745, 1.0010659, [(0.0099481754, 1.0010659)]]\n",
      "85 [0.009018898, 1.0009663, [(0.009018898, 1.0009663)]]\n",
      "86 [0.0081768828, 1.0008761, [(0.0081768828, 1.0008761)]]\n",
      "87 [0.0074131489, 1.0007943, [(0.0074131489, 1.0007943)]]\n",
      "88 [0.0067215757, 1.0007201, [(0.0067215757, 1.0007201)]]\n",
      "89 [0.006094058, 1.0006529, [(0.006094058, 1.0006529)]]\n",
      "90 [0.0055252714, 1.000592, [(0.0055252714, 1.000592)]]\n",
      "91 [0.0050098896, 1.0005368, [(0.0050098896, 1.0005368)]]\n",
      "92 [0.0045425892, 1.0004867, [(0.0045425892, 1.0004867)]]\n",
      "93 [0.0041189194, 1.0004413, [(0.0041189194, 1.0004413)]]\n",
      "94 [0.003733953, 1.0004001, [(0.003733953, 1.0004001)]]\n",
      "95 [0.0033854642, 1.0003628, [(0.0033854642, 1.0003628)]]\n",
      "96 [0.0030694804, 1.0003289, [(0.0030694804, 1.0003289)]]\n",
      "97 [0.0027837753, 1.0002983, [(0.0027837753, 1.0002983)]]\n",
      "98 [0.0025234222, 1.0002704, [(0.0025234222, 1.0002704)]]\n",
      "99 [0.0022875469, 1.0002451, [(0.0022875469, 1.0002451)]]\n"
     ]
    }
   ],
   "source": [
    "#tensorflow 안에서 grdient를 손대고 싶을 떄 사용하는 방법\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(5.)\n",
    "hypothesis = X * W\n",
    "\n",
    "#직접 구한 gradient값\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "\n",
    "#Gradient값을 얻는다.\n",
    "gvs = optimizer.compute_gradients(cost,[W])\n",
    "\n",
    "#Gradient 적용\n",
    "apply_gradient = optimizer.apply_gradients(gvs)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    print(step, sess.run([gradient, W, gvs]))\n",
    "    sess.run(apply_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
