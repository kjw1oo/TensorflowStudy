{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR by NN\n",
    "- binary classification 이므로  logistic regression이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.37186\n",
      "100 0.698672\n",
      "200 0.694658\n",
      "300 0.693622\n",
      "400 0.693302\n",
      "500 0.6932\n",
      "600 0.693166\n",
      "700 0.693155\n",
      "800 0.69315\n",
      "900 0.693148\n",
      "1000 0.693148\n",
      "1100 0.693147\n",
      "1200 0.693147\n",
      "1300 0.693147\n",
      "1400 0.693147\n",
      "1500 0.693147\n",
      "1600 0.693147\n",
      "1700 0.693147\n",
      "1800 0.693147\n",
      "1900 0.693147\n",
      "2000 0.693147\n",
      "2100 0.693147\n",
      "2200 0.693147\n",
      "2300 0.693147\n",
      "2400 0.693147\n",
      "2500 0.693147\n",
      "2600 0.693147\n",
      "2700 0.693147\n",
      "2800 0.693147\n",
      "2900 0.693147\n",
      "3000 0.693147\n",
      "3100 0.693147\n",
      "3200 0.693147\n",
      "3300 0.693147\n",
      "3400 0.693147\n",
      "3500 0.693147\n",
      "3600 0.693147\n",
      "3700 0.693147\n",
      "3800 0.693147\n",
      "3900 0.693147\n",
      "4000 0.693147\n",
      "4100 0.693147\n",
      "4200 0.693147\n",
      "4300 0.693147\n",
      "4400 0.693147\n",
      "4500 0.693147\n",
      "4600 0.693147\n",
      "4700 0.693147\n",
      "4800 0.693147\n",
      "4900 0.693147\n",
      "5000 0.693147\n",
      "5100 0.693147\n",
      "5200 0.693147\n",
      "5300 0.693147\n",
      "5400 0.693147\n",
      "5500 0.693147\n",
      "5600 0.693147\n",
      "5700 0.693147\n",
      "5800 0.693147\n",
      "5900 0.693147\n",
      "6000 0.693147\n",
      "6100 0.693147\n",
      "6200 0.693147\n",
      "6300 0.693147\n",
      "6400 0.693147\n",
      "6500 0.693147\n",
      "6600 0.693147\n",
      "6700 0.693147\n",
      "6800 0.693147\n",
      "6900 0.693147\n",
      "7000 0.693147\n",
      "7100 0.693147\n",
      "7200 0.693147\n",
      "7300 0.693147\n",
      "7400 0.693147\n",
      "7500 0.693147\n",
      "7600 0.693147\n",
      "7700 0.693147\n",
      "7800 0.693147\n",
      "7900 0.693147\n",
      "8000 0.693147\n",
      "8100 0.693147\n",
      "8200 0.693147\n",
      "8300 0.693147\n",
      "8400 0.693147\n",
      "8500 0.693147\n",
      "8600 0.693147\n",
      "8700 0.693147\n",
      "8800 0.693147\n",
      "8900 0.693147\n",
      "9000 0.693147\n",
      "9100 0.693147\n",
      "9200 0.693147\n",
      "9300 0.693147\n",
      "9400 0.693147\n",
      "9500 0.693147\n",
      "9600 0.693147\n",
      "9700 0.693147\n",
      "9800 0.693147\n",
      "9900 0.693147\n",
      "10000 0.693147\n",
      "\n",
      " hypothesis :\n",
      " [[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n",
      " predicted :\n",
      " [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      " accuracy = 0.5\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [4,2])\n",
    "Y = tf.placeholder(tf.float32, shape = [4,1])\n",
    "W = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "# hypothesis using sigmoid\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "# cost\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "#Accuracy\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict ={X: x_data, Y: y_data}))\n",
    "            \n",
    "    #Accuracy report\n",
    "    h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict ={X: x_data, Y: y_data})\n",
    "    print(\"\\n hypothesis :\\n {}\\n predicted :\\n {}\\n accuracy = {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위와같이 단순한 Logistic Regression 으로는 XOR 문제를 해결할수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "- 먼저 x1, x2에 대해서 돌리고 layer1를 output으로 가짐 (이곳이 layer1)\n",
    "- layer1의 출력값에 대해서 다시 hypothesis를 구한다\n",
    "- Weight의 크기를 잘 정해야한다. W의 shape은 [input 갯수, output 갯수]이다 \n",
    "- bias의 크기는 b의 shape = [output의 갯수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.698173\n",
      "100 0.694777\n",
      "200 0.693921\n",
      "300 0.693226\n",
      "400 0.69262\n",
      "500 0.692058\n",
      "600 0.691505\n",
      "700 0.69093\n",
      "800 0.690305\n",
      "900 0.6896\n",
      "1000 0.688783\n",
      "1100 0.687814\n",
      "1200 0.686646\n",
      "1300 0.685217\n",
      "1400 0.683449\n",
      "1500 0.681238\n",
      "1600 0.678445\n",
      "1700 0.674888\n",
      "1800 0.670321\n",
      "1900 0.664413\n",
      "2000 0.656724\n",
      "2100 0.646672\n",
      "2200 0.633515\n",
      "2300 0.616379\n",
      "2400 0.594382\n",
      "2500 0.5669\n",
      "2600 0.533909\n",
      "2700 0.496196\n",
      "2800 0.455283\n",
      "2900 0.413091\n",
      "3000 0.371518\n",
      "3100 0.332116\n",
      "3200 0.295928\n",
      "3300 0.263485\n",
      "3400 0.2349\n",
      "3500 0.210004\n",
      "3600 0.188466\n",
      "3700 0.16989\n",
      "3800 0.153871\n",
      "3900 0.140033\n",
      "4000 0.128045\n",
      "4100 0.117617\n",
      "4200 0.108506\n",
      "4300 0.10051\n",
      "4400 0.0934584\n",
      "4500 0.0872106\n",
      "4600 0.0816496\n",
      "4700 0.0766778\n",
      "4800 0.0722139\n",
      "4900 0.0681899\n",
      "5000 0.0645481\n",
      "5100 0.0612406\n",
      "5200 0.0582259\n",
      "5300 0.0554693\n",
      "5400 0.052941\n",
      "5500 0.0506152\n",
      "5600 0.0484698\n",
      "5700 0.0464857\n",
      "5800 0.0446463\n",
      "5900 0.0429368\n",
      "6000 0.0413449\n",
      "6100 0.0398591\n",
      "6200 0.0384698\n",
      "6300 0.0371681\n",
      "6400 0.0359463\n",
      "6500 0.0347977\n",
      "6600 0.033716\n",
      "6700 0.0326958\n",
      "6800 0.0317322\n",
      "6900 0.0308207\n",
      "7000 0.0299575\n",
      "7100 0.0291388\n",
      "7200 0.0283614\n",
      "7300 0.0276224\n",
      "7400 0.0269191\n",
      "7500 0.026249\n",
      "7600 0.0256099\n",
      "7700 0.0249999\n",
      "7800 0.0244168\n",
      "7900 0.0238592\n",
      "8000 0.0233254\n",
      "8100 0.0228139\n",
      "8200 0.0223234\n",
      "8300 0.0218527\n",
      "8400 0.0214006\n",
      "8500 0.0209661\n",
      "8600 0.0205482\n",
      "8700 0.0201461\n",
      "8800 0.0197587\n",
      "8900 0.0193854\n",
      "9000 0.0190254\n",
      "9100 0.018678\n",
      "9200 0.0183426\n",
      "9300 0.0180187\n",
      "9400 0.0177056\n",
      "9500 0.0174027\n",
      "9600 0.0171097\n",
      "9700 0.0168261\n",
      "9800 0.0165514\n",
      "9900 0.0162852\n",
      "10000 0.0160272\n",
      "\n",
      "hypothesis : \n",
      " [[ 0.01328449]\n",
      " [ 0.98263341]\n",
      " [ 0.9826355 ]\n",
      " [ 0.01557655]]\n",
      "predicted : \n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [4,2])\n",
    "Y = tf.placeholder(tf.float32, shape = [4,1])\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    \n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}))\n",
    "    \n",
    "    h, p, a= sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "    print(\"\\nhypothesis : \\n {}\\npredicted : \\n{}accuracy : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer 여러개 쌓기( Deep NN for XOR)\n",
    "-입출력 갯수보다 많이 시냅스를 만들어내는것을 wide하게 한다고 한다.\n",
    "- 이렇게 깊게 함으로써 deep NN이라고한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.30968\n",
      "100 0.70582\n",
      "200 0.696567\n",
      "300 0.690289\n",
      "400 0.684936\n",
      "500 0.679595\n",
      "600 0.673637\n",
      "700 0.666439\n",
      "800 0.657209\n",
      "900 0.644837\n",
      "1000 0.627692\n",
      "1100 0.603364\n",
      "1200 0.568213\n",
      "1300 0.516371\n",
      "1400 0.440342\n",
      "1500 0.341462\n",
      "1600 0.240555\n",
      "1700 0.161605\n",
      "1800 0.109817\n",
      "1900 0.0778065\n",
      "2000 0.0577773\n",
      "2100 0.0447391\n",
      "2200 0.0358643\n",
      "2300 0.0295678\n",
      "2400 0.0249364\n",
      "2500 0.0214235\n",
      "2600 0.0186885\n",
      "2700 0.0165116\n",
      "2800 0.0147456\n",
      "2900 0.0132896\n",
      "3000 0.0120719\n",
      "3100 0.011041\n",
      "3200 0.0101587\n",
      "3300 0.00939621\n",
      "3400 0.00873162\n",
      "3500 0.008148\n",
      "3600 0.00763188\n",
      "3700 0.00717263\n",
      "3800 0.00676165\n",
      "3900 0.00639195\n",
      "4000 0.00605789\n",
      "4100 0.00575464\n",
      "4200 0.00547829\n",
      "4300 0.00522554\n",
      "4400 0.00499359\n",
      "4500 0.00478006\n",
      "4600 0.00458289\n",
      "4700 0.00440031\n",
      "4800 0.00423081\n",
      "4900 0.00407312\n",
      "5000 0.00392601\n",
      "5100 0.00378853\n",
      "5200 0.0036598\n",
      "5300 0.00353902\n",
      "5400 0.00342551\n",
      "5500 0.00331857\n",
      "5600 0.00321775\n",
      "5700 0.00312253\n",
      "5800 0.00303247\n",
      "5900 0.00294717\n",
      "6000 0.00286624\n",
      "6100 0.00278944\n",
      "6200 0.0027164\n",
      "6300 0.00264685\n",
      "6400 0.00258063\n",
      "6500 0.0025174\n",
      "6600 0.00245705\n",
      "6700 0.00239938\n",
      "6800 0.00234425\n",
      "6900 0.00229141\n",
      "7000 0.00224082\n",
      "7100 0.00219224\n",
      "7200 0.00214566\n",
      "7300 0.00210091\n",
      "7400 0.00205787\n",
      "7500 0.0020165\n",
      "7600 0.00197664\n",
      "7700 0.00193827\n",
      "7800 0.00190128\n",
      "7900 0.00186564\n",
      "8000 0.00183116\n",
      "8100 0.00179794\n",
      "8200 0.00176584\n",
      "8300 0.00173479\n",
      "8400 0.00170475\n",
      "8500 0.0016757\n",
      "8600 0.00164756\n",
      "8700 0.00162032\n",
      "8800 0.00159393\n",
      "8900 0.00156833\n",
      "9000 0.0015435\n",
      "9100 0.00151941\n",
      "9200 0.00149599\n",
      "9300 0.00147331\n",
      "9400 0.00145123\n",
      "9500 0.00142982\n",
      "9600 0.00140895\n",
      "9700 0.00138866\n",
      "9800 0.00136894\n",
      "9900 0.00134971\n",
      "10000 0.00133102\n",
      "\n",
      "hypothesis : \n",
      " [[ 0.00113799]\n",
      " [ 0.99913543]\n",
      " [ 0.99860364]\n",
      " [ 0.00192136]]\n",
      "predicted : \n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [4,2])\n",
    "Y = tf.placeholder(tf.float32, shape = [4,1])\n",
    "#layer1\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "#layer2\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name = 'bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "#layer3\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name = 'weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = 'bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "#layer4\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name = 'weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name = 'bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "    \n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}))\n",
    "    \n",
    "    h, p, a= sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "    print(\"\\nhypothesis : \\n {}\\npredicted : \\n{}accuracy : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise\n",
    "- wide and deep NN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch : 0001 cost = 2.374406074\n",
      "Epoch : 0002 cost = 2.300445251\n",
      "Epoch : 0003 cost = 2.294557711\n",
      "Epoch : 0004 cost = 2.287289383\n",
      "Epoch : 0005 cost = 2.276930635\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#one hot 인코딩 처리 안해도 불러올때 자동으로 onehot 처리 가능\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot = True)\n",
    "\n",
    "nb_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "#layer1\n",
    "W1 = tf.Variable(tf.random_normal([784, 1000]), name = 'weight')\n",
    "b1 = tf.Variable(tf.random_normal([1000]), name = 'bias')\n",
    "layer1 = tf.nn.softmax(tf.matmul(X, W1) + b1)\n",
    "#layer2\n",
    "W2 = tf.Variable(tf.random_normal([1000, 1000]), name = 'weight')\n",
    "b2 = tf.Variable(tf.random_normal([1000]), name = 'bias')\n",
    "layer2 = tf.nn.softmax(tf.matmul(layer1, W2) + b2)\n",
    "#layer3\n",
    "W3 = tf.Variable(tf.random_normal([1000, nb_classes]), name = 'weight')\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]), name = 'bias')\n",
    "hypothesis = tf.nn.softmax(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "#cost function\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict = {X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print(\"Epoch : {:04d} cost = {:.9f}\".format(epoch+1, avg_cost))\n",
    "    #accuracy report\n",
    "    #sess.run없이 돌리는 방법       \n",
    "    print(\"Accuracy : \", accuracy.eval(session = sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label :\", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"prediction:\", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap = 'Greys', interpolation = 'nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Board\n",
    "\n",
    "- deep learning을 할때 복잡한 TF graph를 시각화 할수 있다.\n",
    "- 값들의 plotting이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard의 다섯 단계\n",
    "\n",
    "- 1단계 : TensorFlow의 그래프로부터 나타내고 싶은 Tensor들을 정한다.\n",
    "      w2_hist = tf.summary.histogram(\"weight2\", W2)\n",
    "      cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "- 2단계 : tensor들을 모두 병합한다.\n",
    "        summary = tf.summary.merge_all()\n",
    "- 3단계 : session에서 어디다가 기록할 것인지 파일 이름을 정한다.\n",
    "        writer = tf.summary.FileWriter(' /logs')\n",
    "- 4단계 : 만들어진 summary를 실행하고, 실제로 파일을 여기서 만든다.\n",
    "        s, _ = sess.run([summary, optimizer], feed_dict =feed_dict)\n",
    "        writer.add_summary(s, global_step = global_step\n",
    "       \n",
    "- 5단계 : tensorboard를 실행한다.\n",
    "        tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram( multi-dimensional tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [4,2])\n",
    "Y = tf.placeholder(tf.float32, shape = [4,1])\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    \n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    " \n",
    "#각 layer별로 깔끔하게 정리하기 위한것     \n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    w1_hist = tf.summary.histogram(\"weight1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"bias1\", b1)\n",
    "    layer1_hist = tf.summary.histogram(\"layer1\", layer1)\n",
    "    \n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "    \n",
    "    w2_hist = tf.summary.histogram(\"weight2\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"bias2\", b2)\n",
    "    hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "#summary\n",
    "summary = tf.summary.merge_all()\n",
    "#2, 3단계\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "writer = tf.summary.FileWriter(\"./logs/xor_logs\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "s, _ = sess.run([summary, train], feed_dict={X: x_data, Y: y_data})\n",
    "writer.add_summary(s, global_step=global_step)\n",
    "global_step += 1\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name='x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name='y-input')\n",
    "\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    w1_hist = tf.summary.histogram(\"weights1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"biases1\", b1)\n",
    "    layer1_hist = tf.summary.histogram(\"layer1\", layer1)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    w2_hist = tf.summary.histogram(\"weights2\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"biases2\", b2)\n",
    "    hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                           tf.log(1 - hypothesis))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        summary, _ = sess.run([merged_summary, train], feed_dict={X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={\n",
    "                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
