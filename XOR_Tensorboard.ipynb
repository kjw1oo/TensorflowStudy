{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR by NN\n",
    "- binary classification 이므로  logistic regression이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.710204\n",
      "100 0.700515\n",
      "200 0.696505\n",
      "300 0.694675\n",
      "400 0.693843\n",
      "500 0.693464\n",
      "600 0.693291\n",
      "700 0.693213\n",
      "800 0.693177\n",
      "900 0.693161\n",
      "1000 0.693153\n",
      "1100 0.69315\n",
      "1200 0.693148\n",
      "1300 0.693148\n",
      "1400 0.693147\n",
      "1500 0.693147\n",
      "1600 0.693147\n",
      "1700 0.693147\n",
      "1800 0.693147\n",
      "1900 0.693147\n",
      "2000 0.693147\n",
      "2100 0.693147\n",
      "2200 0.693147\n",
      "2300 0.693147\n",
      "2400 0.693147\n",
      "2500 0.693147\n",
      "2600 0.693147\n",
      "2700 0.693147\n",
      "2800 0.693147\n",
      "2900 0.693147\n",
      "3000 0.693147\n",
      "3100 0.693147\n",
      "3200 0.693147\n",
      "3300 0.693147\n",
      "3400 0.693147\n",
      "3500 0.693147\n",
      "3600 0.693147\n",
      "3700 0.693147\n",
      "3800 0.693147\n",
      "3900 0.693147\n",
      "4000 0.693147\n",
      "4100 0.693147\n",
      "4200 0.693147\n",
      "4300 0.693147\n",
      "4400 0.693147\n",
      "4500 0.693147\n",
      "4600 0.693147\n",
      "4700 0.693147\n",
      "4800 0.693147\n",
      "4900 0.693147\n",
      "5000 0.693147\n",
      "5100 0.693147\n",
      "5200 0.693147\n",
      "5300 0.693147\n",
      "5400 0.693147\n",
      "5500 0.693147\n",
      "5600 0.693147\n",
      "5700 0.693147\n",
      "5800 0.693147\n",
      "5900 0.693147\n",
      "6000 0.693147\n",
      "6100 0.693147\n",
      "6200 0.693147\n",
      "6300 0.693147\n",
      "6400 0.693147\n",
      "6500 0.693147\n",
      "6600 0.693147\n",
      "6700 0.693147\n",
      "6800 0.693147\n",
      "6900 0.693147\n",
      "7000 0.693147\n",
      "7100 0.693147\n",
      "7200 0.693147\n",
      "7300 0.693147\n",
      "7400 0.693147\n",
      "7500 0.693147\n",
      "7600 0.693147\n",
      "7700 0.693147\n",
      "7800 0.693147\n",
      "7900 0.693147\n",
      "8000 0.693147\n",
      "8100 0.693147\n",
      "8200 0.693147\n",
      "8300 0.693147\n",
      "8400 0.693147\n",
      "8500 0.693147\n",
      "8600 0.693147\n",
      "8700 0.693147\n",
      "8800 0.693147\n",
      "8900 0.693147\n",
      "9000 0.693147\n",
      "9100 0.693147\n",
      "9200 0.693147\n",
      "9300 0.693147\n",
      "9400 0.693147\n",
      "9500 0.693147\n",
      "9600 0.693147\n",
      "9700 0.693147\n",
      "9800 0.693147\n",
      "9900 0.693147\n",
      "10000 0.693147\n",
      "\n",
      " hypothesis :\n",
      " [[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n",
      " predicted :\n",
      " [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      " accuracy = 0.5\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "with tf.device('/cpu:0'):\n",
    "    X = tf.placeholder(tf.float32, shape = [4,2])\n",
    "    Y = tf.placeholder(tf.float32, shape = [4,1])\n",
    "    W = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
    "    b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "    # hypothesis using sigmoid\n",
    "    hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "    # cost\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "    #Accuracy\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict ={X: x_data, Y: y_data}))\n",
    "            \n",
    "    #Accuracy report\n",
    "    h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict ={X: x_data, Y: y_data})\n",
    "    print(\"\\n hypothesis :\\n {}\\n predicted :\\n {}\\n accuracy = {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위와같이 단순한 Logistic Regression 으로는 XOR 문제를 해결할수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "- 먼저 x1, x2에 대해서 돌리고 layer1를 output으로 가짐 (이곳이 layer1)\n",
    "- layer1의 출력값에 대해서 다시 hypothesis를 구한다\n",
    "- Weight의 크기를 잘 정해야한다. W의 shape은 [input 갯수, output 갯수]이다 \n",
    "- bias의 크기는 b의 shape = [output의 갯수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.931222\n",
      "100 0.689096\n",
      "200 0.687195\n",
      "300 0.685212\n",
      "400 0.682975\n",
      "500 0.680357\n",
      "600 0.67722\n",
      "700 0.673418\n",
      "800 0.6688\n",
      "900 0.663229\n",
      "1000 0.656595\n",
      "1100 0.648832\n",
      "1200 0.639937\n",
      "1300 0.62996\n",
      "1400 0.618984\n",
      "1500 0.607079\n",
      "1600 0.594251\n",
      "1700 0.580381\n",
      "1800 0.565191\n",
      "1900 0.548228\n",
      "2000 0.528925\n",
      "2100 0.506741\n",
      "2200 0.481378\n",
      "2300 0.452996\n",
      "2400 0.422317\n",
      "2500 0.390506\n",
      "2600 0.358882\n",
      "2700 0.328594\n",
      "2800 0.300428\n",
      "2900 0.274785\n",
      "3000 0.251762\n",
      "3100 0.231259\n",
      "3200 0.213075\n",
      "3300 0.196964\n",
      "3400 0.182681\n",
      "3500 0.169992\n",
      "3600 0.15869\n",
      "3700 0.148591\n",
      "3800 0.139538\n",
      "3900 0.131393\n",
      "4000 0.12404\n",
      "4100 0.11738\n",
      "4200 0.111328\n",
      "4300 0.10581\n",
      "4400 0.100764\n",
      "4500 0.0961362\n",
      "4600 0.0918798\n",
      "4700 0.0879547\n",
      "4800 0.0843259\n",
      "4900 0.0809629\n",
      "5000 0.077839\n",
      "5100 0.0749311\n",
      "5200 0.0722185\n",
      "5300 0.069683\n",
      "5400 0.0673087\n",
      "5500 0.0650813\n",
      "5600 0.0629881\n",
      "5700 0.0610179\n",
      "5800 0.0591605\n",
      "5900 0.057407\n",
      "6000 0.055749\n",
      "6100 0.0541794\n",
      "6200 0.0526915\n",
      "6300 0.0512792\n",
      "6400 0.0499371\n",
      "6500 0.0486603\n",
      "6600 0.0474443\n",
      "6700 0.0462851\n",
      "6800 0.0451786\n",
      "6900 0.0441217\n",
      "7000 0.0431112\n",
      "7100 0.042144\n",
      "7200 0.0412176\n",
      "7300 0.0403294\n",
      "7400 0.0394773\n",
      "7500 0.0386592\n",
      "7600 0.037873\n",
      "7700 0.0371171\n",
      "7800 0.0363897\n",
      "7900 0.0356893\n",
      "8000 0.0350145\n",
      "8100 0.0343637\n",
      "8200 0.0337361\n",
      "8300 0.0331302\n",
      "8400 0.0325449\n",
      "8500 0.0319794\n",
      "8600 0.0314326\n",
      "8700 0.0309036\n",
      "8800 0.0303916\n",
      "8900 0.0298958\n",
      "9000 0.0294154\n",
      "9100 0.0289498\n",
      "9200 0.0284983\n",
      "9300 0.0280603\n",
      "9400 0.0276351\n",
      "9500 0.0272224\n",
      "9600 0.0268214\n",
      "9700 0.0264317\n",
      "9800 0.026053\n",
      "9900 0.0256847\n",
      "10000 0.0253263\n",
      "\n",
      "hypothesis : \n",
      " [[ 0.02244969]\n",
      " [ 0.97126234]\n",
      " [ 0.97079527]\n",
      " [ 0.01960655]]\n",
      "predicted : \n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "with tf.device('/cpu:0'): \n",
    "    X = tf.placeholder(tf.float32, shape = [4,2])\n",
    "    Y = tf.placeholder(tf.float32, shape = [4,1])\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "   \n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}))\n",
    "    \n",
    "    h, p, a= sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "    print(\"\\nhypothesis : \\n {}\\npredicted : \\n{}accuracy : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer 여러개 쌓기( Deep NN for XOR)\n",
    "-입출력 갯수보다 많이 시냅스를 만들어내는것을 wide하게 한다고 한다.\n",
    "- 이렇게 깊게 함으로써 deep NN이라고한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.725778\n",
      "100 0.677054\n",
      "200 0.668924\n",
      "300 0.657683\n",
      "400 0.642251\n",
      "500 0.621514\n",
      "600 0.594051\n",
      "700 0.55758\n",
      "800 0.508244\n",
      "900 0.44138\n",
      "1000 0.356763\n",
      "1100 0.266385\n",
      "1200 0.189398\n",
      "1300 0.134497\n",
      "1400 0.0982953\n",
      "1500 0.0745741\n",
      "1600 0.0586217\n",
      "1700 0.0475075\n",
      "1800 0.0394866\n",
      "1900 0.0335107\n",
      "2000 0.0289329\n",
      "2100 0.0253409\n",
      "2200 0.0224637\n",
      "2300 0.0201176\n",
      "2400 0.0181748\n",
      "2500 0.0165442\n",
      "2600 0.0151593\n",
      "2700 0.0139708\n",
      "2800 0.0129413\n",
      "2900 0.0120421\n",
      "3000 0.0112509\n",
      "3100 0.01055\n",
      "3200 0.00992547\n",
      "3300 0.00936576\n",
      "3400 0.00886168\n",
      "3500 0.00840556\n",
      "3600 0.00799113\n",
      "3700 0.00761309\n",
      "3800 0.00726702\n",
      "3900 0.00694916\n",
      "4000 0.00665621\n",
      "4100 0.00638556\n",
      "4200 0.00613474\n",
      "4300 0.00590176\n",
      "4400 0.00568481\n",
      "4500 0.00548238\n",
      "4600 0.00529304\n",
      "4700 0.00511564\n",
      "4800 0.00494911\n",
      "4900 0.00479245\n",
      "5000 0.00464493\n",
      "5100 0.00450568\n",
      "5200 0.00437418\n",
      "5300 0.00424969\n",
      "5400 0.00413174\n",
      "5500 0.00401982\n",
      "5600 0.0039135\n",
      "5700 0.00381238\n",
      "5800 0.00371611\n",
      "5900 0.00362432\n",
      "6000 0.00353675\n",
      "6100 0.00345307\n",
      "6200 0.00337311\n",
      "6300 0.00329659\n",
      "6400 0.00322326\n",
      "6500 0.00315302\n",
      "6600 0.00308558\n",
      "6700 0.00302088\n",
      "6800 0.00295871\n",
      "6900 0.00289888\n",
      "7000 0.00284135\n",
      "7100 0.00278595\n",
      "7200 0.00273254\n",
      "7300 0.0026811\n",
      "7400 0.00263146\n",
      "7500 0.00258353\n",
      "7600 0.00253724\n",
      "7700 0.00249252\n",
      "7800 0.00244926\n",
      "7900 0.00240744\n",
      "8000 0.00236694\n",
      "8100 0.00232771\n",
      "8200 0.00228973\n",
      "8300 0.00225292\n",
      "8400 0.00221723\n",
      "8500 0.00218256\n",
      "8600 0.00214894\n",
      "8700 0.00211632\n",
      "8800 0.00208459\n",
      "8900 0.00205376\n",
      "9000 0.00202382\n",
      "9100 0.00199467\n",
      "9200 0.00196636\n",
      "9300 0.00193881\n",
      "9400 0.00191195\n",
      "9500 0.00188582\n",
      "9600 0.00186033\n",
      "9700 0.00183552\n",
      "9800 0.00181133\n",
      "9900 0.00178776\n",
      "10000 0.00176477\n",
      "\n",
      "hypothesis : \n",
      " [[ 0.00240161]\n",
      " [ 0.99823725]\n",
      " [ 0.99821055]\n",
      " [ 0.00109862]]\n",
      "predicted : \n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "with tf.device('/cpu:0'): \n",
    "    X = tf.placeholder(tf.float32, shape = [4,2])\n",
    "    Y = tf.placeholder(tf.float32, shape = [4,1])\n",
    "    #layer1\n",
    "    W1 = tf.Variable(tf.random_normal([2, 10]), name = 'weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([10]), name = 'bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    #layer2\n",
    "    W2 = tf.Variable(tf.random_normal([10, 10]), name = 'weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([10]), name = 'bias2')\n",
    "    layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "    #layer3\n",
    "    W3 = tf.Variable(tf.random_normal([10, 10]), name = 'weight3')\n",
    "    b3 = tf.Variable(tf.random_normal([10]), name = 'bias3')\n",
    "    layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "    #layer4\n",
    "    W4 = tf.Variable(tf.random_normal([10, 1]), name = 'weight4')\n",
    "    b4 = tf.Variable(tf.random_normal([1]), name = 'bias4')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}))\n",
    "    \n",
    "    h, p, a= sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "    print(\"\\nhypothesis : \\n {}\\npredicted : \\n{}accuracy : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise\n",
    "- wide and deep NN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch : 0001 cost = 2.333859217\n",
      "Epoch : 0002 cost = 2.290696158\n",
      "Epoch : 0003 cost = 2.279023378\n",
      "Epoch : 0004 cost = 2.261321471\n",
      "Epoch : 0005 cost = 2.231532294\n",
      "Epoch : 0006 cost = 2.168082341\n",
      "Epoch : 0007 cost = 2.032105989\n",
      "Epoch : 0008 cost = 1.885276770\n",
      "Epoch : 0009 cost = 1.783182018\n",
      "Epoch : 0010 cost = 1.707145279\n",
      "Epoch : 0011 cost = 1.629309946\n",
      "Epoch : 0012 cost = 1.534887178\n",
      "Epoch : 0013 cost = 1.468797215\n",
      "Epoch : 0014 cost = 1.423404175\n",
      "Epoch : 0015 cost = 1.388750117\n",
      "Accuracy :  0.4523\n",
      "Label : [8]\n",
      "prediction: [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPxAU4QeCIvxAUIQfCIrwA0ERfiCo/wfvxlDskXa9OAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x239c4dddd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#one hot 인코딩 처리 안해도 불러올때 자동으로 onehot 처리 가능\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot = True)\n",
    "with tf.device('/cpu:0'): \n",
    "    nb_classes = 10\n",
    "    X = tf.placeholder(tf.float32, [None, 784])\n",
    "    Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "    #layer1\n",
    "    W1 = tf.Variable(tf.random_normal([784, 1000]), name = 'weight')\n",
    "    b1 = tf.Variable(tf.random_normal([1000]), name = 'bias')\n",
    "    layer1 = tf.nn.softmax(tf.matmul(X, W1) + b1)\n",
    "    #layer2\n",
    "    W2 = tf.Variable(tf.random_normal([1000, 1000]), name = 'weight')\n",
    "    b2 = tf.Variable(tf.random_normal([1000]), name = 'bias')\n",
    "    layer2 = tf.nn.softmax(tf.matmul(layer1, W2) + b2)\n",
    "    #layer3\n",
    "    W3 = tf.Variable(tf.random_normal([1000, nb_classes]), name = 'weight')\n",
    "    b3 = tf.Variable(tf.random_normal([nb_classes]), name = 'bias')\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "    #cost function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict = {X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print(\"Epoch : {:04d} cost = {:.9f}\".format(epoch+1, avg_cost))\n",
    "    #accuracy report\n",
    "    #sess.run없이 돌리는 방법       \n",
    "    print(\"Accuracy : \", accuracy.eval(session = sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label :\", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"prediction:\", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap = 'Greys', interpolation = 'nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Board\n",
    "\n",
    "- deep learning을 할때 복잡한 TF graph를 시각화 할수 있다.\n",
    "- 값들의 plotting이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard의 다섯 단계\n",
    "\n",
    "- 1단계 : TensorFlow의 그래프로부터 나타내고 싶은 Tensor들을 정한다.\n",
    "      w2_hist = tf.summary.histogram(\"weight2\", W2)\n",
    "      cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "- 2단계 : tensor들을 모두 병합한다.\n",
    "        summary = tf.summary.merge_all()\n",
    "- 3단계 : session에서 어디다가 기록할 것인지 파일 이름을 정한다.\n",
    "        writer = tf.summary.FileWriter(' /logs')\n",
    "- 4단계 : 만들어진 summary를 실행하고, 실제로 파일을 여기서 만든다.\n",
    "        s, _ = sess.run([summary, optimizer], feed_dict =feed_dict)\n",
    "        writer.add_summary(s, global_step = global_step\n",
    "       \n",
    "- 5단계 : tensorboard를 실행한다.\n",
    "        tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram( multi-dimensional tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_8' with dtype float and shape [4,2]\n\t [[Node: Placeholder_8 = Placeholder[dtype=DT_FLOAT, shape=[4,2], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: layer2/Sigmoid/_41 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_398_layer2/Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder_8', defined at:\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-445342b2a344>\", line 4, in <module>\n    X = tf.placeholder(tf.float32, shape = [4,2])\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1507, in placeholder\n    name=name)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1997, in _placeholder\n    name=name)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_8' with dtype float and shape [4,2]\n\t [[Node: Placeholder_8 = Placeholder[dtype=DT_FLOAT, shape=[4,2], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: layer2/Sigmoid/_41 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_398_layer2/Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_8' with dtype float and shape [4,2]\n\t [[Node: Placeholder_8 = Placeholder[dtype=DT_FLOAT, shape=[4,2], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: layer2/Sigmoid/_41 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_398_layer2/Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-db0940810b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_8' with dtype float and shape [4,2]\n\t [[Node: Placeholder_8 = Placeholder[dtype=DT_FLOAT, shape=[4,2], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: layer2/Sigmoid/_41 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_398_layer2/Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder_8', defined at:\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-445342b2a344>\", line 4, in <module>\n    X = tf.placeholder(tf.float32, shape = [4,2])\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1507, in placeholder\n    name=name)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1997, in _placeholder\n    name=name)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_8' with dtype float and shape [4,2]\n\t [[Node: Placeholder_8 = Placeholder[dtype=DT_FLOAT, shape=[4,2], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: layer2/Sigmoid/_41 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_398_layer2/Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "with tf.device('/cpu:0'): \n",
    "    X = tf.placeholder(tf.float32, shape = [4,2])\n",
    "    Y = tf.placeholder(tf.float32, shape = [4,1])\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "    #각 layer별로 깔끔하게 정리하기 위한것     \n",
    "    with tf.name_scope(\"layer1\") as scope:\n",
    "        W1 = tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "        b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "        layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "        w1_hist = tf.summary.histogram(\"weight1\", W1)\n",
    "        b1_hist = tf.summary.histogram(\"bias1\", b1)\n",
    "        layer1_hist = tf.summary.histogram(\"layer1\", layer1)\n",
    "\n",
    "    with tf.name_scope(\"layer2\") as scope:\n",
    "        W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "        b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "        w2_hist = tf.summary.histogram(\"weight2\", W2)\n",
    "        b2_hist = tf.summary.histogram(\"bias2\", b2)\n",
    "        hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "    #summary\n",
    "    summary = tf.summary.merge_all()\n",
    "#2, 3단계\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "writer = tf.summary.FileWriter(\"./logs/xor_logs\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "s, _ = sess.run([summary, train], feed_dict={X: x_data, Y: y_data})\n",
    "writer.add_summary(s, global_step=global_step)\n",
    "global_step += 1\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "x_data = np.array(x_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name='x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name='y-input')\n",
    "\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    w1_hist = tf.summary.histogram(\"weights1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"biases1\", b1)\n",
    "    layer1_hist = tf.summary.histogram(\"layer1\", layer1)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    w2_hist = tf.summary.histogram(\"weights2\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"biases2\", b2)\n",
    "    hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                           tf.log(1 - hypothesis))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        summary, _ = sess.run([merged_summary, train], feed_dict={X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={\n",
    "                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
